{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune `meta-llama/Meta-Llama-3-8B-Instruct` on an EC2 instance using `Unsloth`\n",
    "---\n",
    "\n",
    "Unsloth makes finetuning large language models like Llama-3, Mistral, Phi-4 and Gemma 2x faster, use 70% less memory, and with no degradation in accuracy!\n",
    "\n",
    "**Note**: ***This notebook is run on a `g6e.12xlarge` instance. Follow the prerequisite steps [here](README.md)***\n",
    "\n",
    "In this example, we will be fine tuning the llama3 8b instruct model. There are several 4bit pre quantized models that `unsloth` provides that are not gated. This supports 4x faster downloading with no OOMs. In this case, we will be using the standard `meta-llama/Meta-Llama-3-8B-Instruct` model from hugging face. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spring-2025-lab07-Sprerana/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 00:39:27,864\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import getpass\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import standardize_sharegpt\n",
    "from unsloth import apply_chat_template\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Remove existing handlers\n",
    "if logger.handlers:\n",
    "   logger.handlers.clear()\n",
    "\n",
    "\n",
    "# Add a simple handler\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:39:28,691] p35048 {944205494.py:11} INFO - Using model: meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "   os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your HuggingFace token: \")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Set model ID - Ensure this exactly matches the Hugging Face repository name\n",
    "hf_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "logger.info(f\"Using model: {hf_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:39:28,696] p35048 {1483811561.py:9} INFO - Using token prefix: hf_T*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_T\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "max_seq_length = 2048  \n",
    "dtype = None \n",
    "load_in_4bit = False\n",
    "\n",
    "\n",
    "# Print token info just to make sure\n",
    "token_prefix = hf_token[:4] + \"*\" * (len(hf_token) - 4) if hf_token else \"None\"\n",
    "logger.info(f\"Using token prefix: {token_prefix}\")\n",
    "print(hf_token[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:39:28,702] p35048 {2742727994.py:3} INFO - Attempting to load model from meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.045 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:39:37,320] p35048 {2742727994.py:11} INFO - Successfully loaded model and tokenizer\n",
      "Unsloth 2025.2.15 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "try:\n",
    "   logger.info(f\"Attempting to load model from {hf_model_id}\")\n",
    "   model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "       model_name=hf_model_id,\n",
    "       max_seq_length=max_seq_length,\n",
    "       dtype=dtype,\n",
    "       load_in_4bit=load_in_4bit,\n",
    "       token=hf_token\n",
    "   )\n",
    "   logger.info(f\"Successfully loaded model and tokenizer\")\n",
    "except Exception as e:\n",
    "   logger.error(f\"Error occurred while loading the model: {e}\")\n",
    "   logger.error(\"Please ensure you have accepted the model license on Hugging Face\")\n",
    "   logger.error(\"Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to accept\")\n",
    "   raise\n",
    "\n",
    "\n",
    "# Prepare the model for fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "   model,\n",
    "   r=16,# Low-rank adaptation parameter\n",
    "   target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "   lora_alpha=16,\n",
    "   lora_dropout=0,  \n",
    "   bias=\"none\",     \n",
    "   use_gradient_checkpointing=\"unsloth\", \n",
    "   random_state=3407,\n",
    "   use_rslora=False,  \n",
    "   loftq_config=None, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "Using Banking77 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:39:39,826] p35048 {940285120.py:2} INFO - Loading Banking77 dataset\n",
      "[2025-03-20 00:39:43,005] p35048 {940285120.py:4} INFO - Columns in the dataset: ['text', 'label']\n",
      "[2025-03-20 00:39:43,005] p35048 {940285120.py:5} INFO - Dataset splits: dict_keys(['train', 'test'])\n",
      "[2025-03-20 00:39:43,006] p35048 {940285120.py:6} INFO - Training examples: 10003\n",
      "[2025-03-20 00:39:43,006] p35048 {940285120.py:9} INFO - Getting category names from the dataset\n",
      "[2025-03-20 00:39:43,006] p35048 {940285120.py:11} INFO - Number of categories: 77\n",
      "[2025-03-20 00:39:43,007] p35048 {940285120.py:12} INFO - First 5 category names: ['activate_my_card', 'age_limit', 'apple_pay_or_google_pay', 'atm_support', 'automatic_top_up']\n"
     ]
    }
   ],
   "source": [
    "# Load the Banking77 dataset\n",
    "logger.info(\"Loading Banking77 dataset\")\n",
    "dataset = load_dataset(\"banking77\")\n",
    "logger.info(f\"Columns in the dataset: {dataset['train'].column_names}\")\n",
    "logger.info(f\"Dataset splits: {dataset.keys()}\")\n",
    "logger.info(f\"Training examples: {len(dataset['train'])}\")\n",
    "\n",
    "# Get the label names of the Banking77 dataset\n",
    "logger.info(\"Getting category names from the dataset\")\n",
    "category_names = dataset[\"train\"].features[\"label\"].names\n",
    "logger.info(f\"Number of categories: {len(category_names)}\")\n",
    "logger.info(f\"First 5 category names: {category_names[:5]}\")\n",
    "\n",
    "# Convert the dataset to the appropriate format\n",
    "def format_banking77(examples):\n",
    "   formatted_conversations = []\n",
    "  \n",
    "   for text, label in zip(examples[\"text\"], examples[\"label\"]):\n",
    "       # Include both the label number and name in the training\n",
    "       category_name = category_names[label]\n",
    "       conversation = [\n",
    "           {\"role\": \"user\", \"content\": f\"Classify the following banking query into the correct category: {text}\"},\n",
    "           {\"role\": \"assistant\", \"content\": f\"Category {label}: {category_name}\"}\n",
    "       ]\n",
    "       formatted_conversations.append(conversation)\n",
    "  \n",
    "   return {\"conversations\": formatted_conversations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:39:43,012] p35048 {3104140645.py:2} INFO - Formatting training dataset\n",
      "[2025-03-20 00:39:43,015] p35048 {3104140645.py:11} INFO - Standardizing dataset\n"
     ]
    }
   ],
   "source": [
    "# Apply formatting to the training set\n",
    "logger.info(\"Formatting training dataset\")\n",
    "train_formatted = dataset[\"train\"].map(\n",
    "   format_banking77,\n",
    "   batched=True,\n",
    "   remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "\n",
    "# Now standardize the dataset\n",
    "logger.info(\"Standardizing dataset\")\n",
    "standardized_dataset = standardize_sharegpt(train_formatted)\n",
    "\n",
    "\n",
    "# Define the chat template\n",
    "chat_template = \"\"\"Below is a banking query. Classify it into the appropriate category number (0-76) and provide the category name.\n",
    "\n",
    "\n",
    "### Instruction:\n",
    "{INPUT}\n",
    "\n",
    "\n",
    "### Response:\n",
    "{OUTPUT}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:39:43,023] p35048 {2532199462.py:2} INFO - Applying chat template\n",
      "Unsloth: We automatically added an EOS token to stop endless generations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10003/10003 [00:00<00:00, 30719.44 examples/s]\n",
      "[2025-03-20 00:39:43,514] p35048 {2532199462.py:15} INFO - Setting up trainer\n",
      "Converting train dataset to ChatML (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10003/10003 [00:00<00:00, 25681.62 examples/s]\n",
      "Applying chat template to train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10003/10003 [00:01<00:00, 7735.72 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10003/10003 [00:01<00:00, 5343.42 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10003/10003 [00:00<00:00, 16296.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the chat template\n",
    "logger.info(\"Applying chat template\")\n",
    "processed_dataset = apply_chat_template(\n",
    "   standardized_dataset,\n",
    "   tokenizer=tokenizer,\n",
    "   chat_template=chat_template,\n",
    ")\n",
    "\n",
    "\n",
    "# Set up the training\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "logger.info(\"Setting up trainer\")\n",
    "trainer = SFTTrainer(\n",
    "   model=model,\n",
    "   tokenizer=tokenizer,\n",
    "   train_dataset=processed_dataset,\n",
    "   dataset_text_field=\"text\",\n",
    "   max_seq_length=max_seq_length,\n",
    "   dataset_num_proc=2,\n",
    "   packing=False,  # Can make training 5x faster for short sequences.\n",
    "   args=TrainingArguments(\n",
    "       per_device_train_batch_size=4,  # Adjust based on your GPU\n",
    "       gradient_accumulation_steps=4,\n",
    "       warmup_steps=100,\n",
    "       max_steps=600,  # As requested\n",
    "       num_train_epochs=1,  # Run for 1 epoch as requested\n",
    "       learning_rate=2e-4,\n",
    "       fp16=not is_bfloat16_supported(),\n",
    "       bf16=is_bfloat16_supported(),\n",
    "       logging_steps=10,\n",
    "       optim=\"adamw_8bit\",\n",
    "       weight_decay=0.01,\n",
    "       lr_scheduler_type=\"linear\",\n",
    "       seed=3407,\n",
    "       output_dir=\"outputs\",\n",
    "       report_to=\"none\",  # Use this for WandB etc\n",
    "   ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:39:48,320] p35048 {840474025.py:2} INFO - Starting training\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 10,003 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 600\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 05:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.567300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.561800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.439600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.921700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.879700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.839700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.713200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.670600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.609400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.640300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.608700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.614600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.614500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.600800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.593400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.590400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.554300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.498300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.525800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.568800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.480100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.509800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.464300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.503400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.470800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.517100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.476500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.503200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "logger.info(\"Starting training\")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:44:50,788] p35048 {3055148779.py:15} INFO - Training Statistics:\n",
      "Global Steps: 600\n",
      "Training Loss: 0.7399\n",
      "\n",
      "\n",
      "Metrics:\n",
      "- Train Runtime: 301.565 seconds\n",
      "- Training Samples/Second: 31.834\n",
      "- Training Steps/Second: 1.990\n",
      "- Total FLOPS: 4.35e+15\n",
      "- Final Train Loss: 0.7399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Format and print training stats\n",
    "# Format the training stats in a readable way\n",
    "output_text = f\"\"\"Training Statistics:\n",
    "Global Steps: {trainer_stats.global_step}\n",
    "Training Loss: {trainer_stats.training_loss:.4f}\n",
    "\n",
    "\n",
    "Metrics:\n",
    "- Train Runtime: {trainer_stats.metrics['train_runtime']:.3f} seconds\n",
    "- Training Samples/Second: {trainer_stats.metrics['train_samples_per_second']:.3f}\n",
    "- Training Steps/Second: {trainer_stats.metrics['train_steps_per_second']:.3f}\n",
    "- Total FLOPS: {trainer_stats.metrics['total_flos']:.2e}\n",
    "- Final Train Loss: {trainer_stats.metrics['train_loss']:.4f}\n",
    "\"\"\"\n",
    "logger.info(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:44:50,793] p35048 {1598851975.py:2} INFO - Saving model and tokenizer\n",
      "[2025-03-20 00:44:51,201] p35048 {1598851975.py:10} INFO - Saved category names to banking77_categories.json\n",
      "[2025-03-20 00:44:51,201] p35048 {1598851975.py:14} INFO - Preparing model for inference\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "logger.info(\"Saving model and tokenizer\")\n",
    "model.save_pretrained(\"banking77_fine_tuned\")\n",
    "tokenizer.save_pretrained(\"banking77_fine_tuned\")\n",
    "\n",
    "# Save the category names for inference\n",
    "import json\n",
    "with open(\"banking77_categories.json\", \"w\") as f:\n",
    "    json.dump(category_names, f)\n",
    "logger.info(\"Saved category names to banking77_categories.json\")\n",
    "\n",
    "\n",
    "# Prepare the model for inference\n",
    "logger.info(\"Preparing model for inference\")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries\n",
    "test_queries = [\n",
    "   \"I see a charge on my credit card statement but I paid on time, why?\",\n",
    "   \"Do you have a branch in Timbuktu?\",\n",
    "   \"I lost my card and my replacement card has not arrived.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response without streaming\n",
    "def generate_response(query):\n",
    "   logger.info(f\"Generating response for: {query}\")\n",
    "   messages = [\n",
    "       {\"role\": \"user\", \"content\": f\"Classify the following banking query into the correct category: {query}\"}\n",
    "   ]\n",
    "   input_ids = tokenizer.apply_chat_template(\n",
    "       messages,\n",
    "       add_generation_prompt=True,\n",
    "       return_tensors=\"pt\",\n",
    "   ).to(\"cuda\")\n",
    "  \n",
    "   outputs = model.generate(\n",
    "       input_ids,\n",
    "       max_new_tokens=128,\n",
    "       pad_token_id=tokenizer.eos_token_id,\n",
    "       do_sample=False\n",
    "   )\n",
    "  \n",
    "   # Decode the output, skipping the prompt\n",
    "   decoded_output = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "   \n",
    "   # Try to parse the output to add category name if it's not already included\n",
    "   try:\n",
    "       # Check if the response is just a number\n",
    "       if decoded_output.strip().isdigit():\n",
    "           category_num = int(decoded_output.strip())\n",
    "           if 0 <= category_num < len(category_names):\n",
    "               return f\"Category {category_num}: {category_names[category_num]}\"\n",
    "       \n",
    "       # If the model already outputs the full format or something else, return as is\n",
    "       return decoded_output.strip()\n",
    "   except:\n",
    "       # If parsing fails, return original output\n",
    "       return decoded_output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:44:51,222] p35048 {3359099013.py:3} INFO - Generating response for: I see a charge on my credit card statement but I paid on time, why?\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "[2025-03-20 00:44:51,538] p35048 {3359099013.py:3} INFO - Generating response for: Do you have a branch in Timbuktu?\n",
      "[2025-03-20 00:44:51,681] p35048 {3359099013.py:3} INFO - Generating response for: I lost my card and my replacement card has not arrived.\n"
     ]
    }
   ],
   "source": [
    "# Generate responses for all test queries\n",
    "results = []\n",
    "for query in test_queries:\n",
    "   response = generate_response(query)\n",
    "   results.append(f\"Input: {query}\\nClassification: {response}\\n\")\n",
    "\n",
    "\n",
    "# Save the results to a file\n",
    "with open(\"problem1_task1.txt\", \"w\") as f:\n",
    "   f.write(\"\\n\".join(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv env)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
